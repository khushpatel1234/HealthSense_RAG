{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzQeGMxgN96y"
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers faiss-cpu biopython"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu --no-cache-dir\n",
        "!pip install sentence-transformers biopython"
      ],
      "metadata": {
        "id": "h3UYUOZPTux9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade protobuf==3.20.3"
      ],
      "metadata": {
        "id": "sHyZqdjRTxre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(new_session=False)\n",
        "# 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   TOKEN"
      ],
      "metadata": {
        "id": "_hywnaF-jREs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "HF_MODEL_ID = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "MODEL_DIR = \"/meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "# Download tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    HF_MODEL_ID,\n",
        "    use_fast=True\n",
        ")\n",
        "\n",
        "# Download model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    HF_MODEL_ID,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=None,\n",
        "    low_cpu_mem_usage=True,\n",
        ")\n",
        "\n",
        "# Save to Google Drive\n",
        "tokenizer.save_pretrained(MODEL_DIR)\n",
        "model.save_pretrained(MODEL_DIR)\n",
        "\n",
        "print(\"Model downloaded and stored at:\", MODEL_DIR)\n"
      ],
      "metadata": {
        "id": "yGhP_ZuRjR2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import faiss\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from Bio import Entrez\n",
        "\n",
        "Entrez.email = \"khushpatel1080@gmail.com\"\n",
        "\n",
        "\n",
        "EMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qulGf9waTxtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class Embedder:\n",
        "    def __init__(self, model_name: str = EMBED_MODEL_NAME):\n",
        "        # SentenceTransformers normalizes embeddings if asked; we'll normalize here explicitly\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "\n",
        "    def encode(self, texts: List[str]) -> np.ndarray:\n",
        "        emb = self.model.encode(\n",
        "            texts,\n",
        "            convert_to_numpy=True,\n",
        "            normalize_embeddings=True,  # cosine similarity via inner product\n",
        "        )\n",
        "        return emb.astype(\"float32\")\n"
      ],
      "metadata": {
        "id": "GhjhTMlsTx0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bio"
      ],
      "metadata": {
        "id": "QoRc-MUMTx2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class PubMedClient:\n",
        "    def __init__(self, max_results: int = 5):\n",
        "        self.max_results = max_results\n",
        "\n",
        "    def fetch_docs(self, query: str) -> List[Dict[str, Any]]:\n",
        "        # 1) ESearch: get PubMed IDs\n",
        "        handle = Entrez.esearch(\n",
        "            db=\"pubmed\",\n",
        "            term=query,\n",
        "            retmax=self.max_results,\n",
        "            sort=\"relevance\",\n",
        "            retmode=\"xml\",\n",
        "        )\n",
        "        search_record = Entrez.read(handle)\n",
        "        ids = search_record.get(\"IdList\", [])\n",
        "        handle.close()\n",
        "\n",
        "        if not ids:\n",
        "            return []\n",
        "\n",
        "        handle = Entrez.efetch(\n",
        "            db=\"pubmed\",\n",
        "            id=\",\".join(ids),\n",
        "            rettype=\"abstract\",\n",
        "            retmode=\"text\",\n",
        "        )\n",
        "        raw_text = handle.read()\n",
        "        handle.close()\n",
        "\n",
        "        chunks = [c.strip() for c in raw_text.split(\"\\n\\n\") if c.strip()]\n",
        "        docs = []\n",
        "        for pmid, chunk in zip(ids, chunks):\n",
        "            docs.append(\n",
        "                {\n",
        "                    \"id\": pmid,\n",
        "                    \"title\": \"\",\n",
        "                    \"abstract\": chunk,\n",
        "                }\n",
        "            )\n",
        "        return docs\n"
      ],
      "metadata": {
        "id": "GDCqywQ2Tx48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from Bio import Entrez\n",
        "\n",
        "class PDFDownloader:\n",
        "    def __init__(self, download_dir=\"/kaggle/working/downloaded_pdfs\"):\n",
        "        self.download_dir = download_dir\n",
        "        os.makedirs(self.download_dir, exist_ok=True)\n",
        "\n",
        "    def get_pmc_id(self, pmid):\n",
        "        \"\"\"Converts a PubMed ID (PMID) to a PubMed Central ID (PMCID) for Open Access download.\"\"\"\n",
        "        try:\n",
        "            handle = Entrez.elink(dbfrom=\"pubmed\", db=\"pmc\", linkname=\"pubmed_pmc\", id=pmid)\n",
        "            result = Entrez.read(handle)\n",
        "            handle.close()\n",
        "            # Extract PMC ID if a link exists\n",
        "            if result and result[0]['LinkSetDb']:\n",
        "                return result[0]['LinkSetDb'][0]['Link'][0]['Id']\n",
        "        except Exception as e:\n",
        "            print(f\"Could not find PMC ID for PMID {pmid}: {e}\")\n",
        "        return None\n",
        "\n",
        "    def download_pdf(self, pmid, title):\n",
        "        \"\"\"Attempts to download the PDF if a PMC ID is found, skipping if file exists.\"\"\"\n",
        "\n",
        "        # Check if file already exists (skipping logic not fully shown in your paste, but good practice)\n",
        "        safe_title = \"\".join([c for c in title if c.isalnum() or c in (' ','-')]).rstrip()\n",
        "        pmc_id_placeholder = \"NO_PMC\" # Placeholder until we know the real ID\n",
        "        filename = f\"{self.download_dir}/{safe_title[:50]}_{pmc_id_placeholder}.pdf\"\n",
        "\n",
        "        # 1. ATTEMPT TO GET PMC ID\n",
        "        pmc_id = self.get_pmc_id(pmid)\n",
        "\n",
        "        if not pmc_id:\n",
        "            print(f\"PDF download skipped for '{title[:30]}...' (No PMC ID found for PMID {pmid}). The article is likely **not archived in PubMed Central (PMC)**.\")\n",
        "            return\n",
        "\n",
        "        # Update filename with actual PMCID\n",
        "        filename = f\"{self.download_dir}/{safe_title[:50]}_PMC{pmc_id}.pdf\"\n",
        "\n",
        "        # 2. CHECK FOR EXISTING FILE\n",
        "        if os.path.exists(filename):\n",
        "            print(f\"[SKIP-2] PDF already in storage: {filename}\")\n",
        "            return\n",
        "\n",
        "        # 3. ATTEMPT TO DOWNLOAD OPEN ACCESS PDF\n",
        "        pdf_url = f\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC{pmc_id}/pdf/\"\n",
        "\n",
        "        try:\n",
        "            headers = {'User-Agent': 'Mozilla/5.0 (ColabUser; mailto:khushpatel1080@gmail.com)'}\n",
        "            response = requests.get(pdf_url, headers=headers, stream=True)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                with open(filename, 'wb') as f:\n",
        "                    f.write(response.content)\n",
        "                print(f\"[SUCCESS] Downloaded: {filename}\")\n",
        "            else:\n",
        "                # Most common failure reason is a 403 Forbidden for paywalled content\n",
        "                print(f\"[FAIL-3] Could not retrieve Open Access PDF for PMC{pmc_id}. Status: {response.status_code}. **The article is likely paywalled.** URL: {pdf_url}\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] downloading PMC{pmc_id}: {e}\")"
      ],
      "metadata": {
        "id": "Inj6Mmc0Tx7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "\n",
        "reward_model = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n"
      ],
      "metadata": {
        "id": "E7ObuKL1sUhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_top3_docs(query: str, docs: list, reward_model) -> list:\n",
        "    \"\"\"\n",
        "    docs: List of dicts with 'text', 'title', 'external_id'\n",
        "    Returns top-3 docs based on reward model scores\n",
        "    \"\"\"\n",
        "    if len(docs) <= 3:\n",
        "        return docs  # nothing to select\n",
        "\n",
        "    pairs = [(query, doc['text']) for doc in docs]\n",
        "    scores = reward_model.predict(pairs)\n",
        "    top3_idx = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:3]\n",
        "    top3_docs = [docs[i] for i in top3_idx]\n",
        "    return top3_docs\n"
      ],
      "metadata": {
        "id": "s08pDSHgsVqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZsPsudgOsZ8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class DocumentStore:\n",
        "    def __init__(self, embedder: Embedder, dim: int):\n",
        "        self.embedder = embedder\n",
        "        self.dim = dim\n",
        "\n",
        "\n",
        "        self.keyword_index = faiss.IndexIDMap(faiss.IndexFlatIP(dim))\n",
        "        self.doc_index = faiss.IndexIDMap(faiss.IndexFlatIP(dim))\n",
        "\n",
        "\n",
        "        self.meta: Dict[int, Dict[str, Any]] = {}\n",
        "\n",
        "        self._next_local_id = 1\n",
        "\n",
        "    def _alloc_id(self) -> int:\n",
        "        doc_id = self._next_local_id\n",
        "        self._next_local_id += 1\n",
        "        return doc_id\n",
        "\n",
        "    def add_document(self, text: str, keyword_text: str, external_id: str = None) -> int:\n",
        "        \"\"\"\n",
        "        text         : full document text (e.g., title + abstract)\n",
        "        keyword_text : the query / keywords used to fetch this doc\n",
        "        external_id  : optional external ID (e.g., PubMed PMID)\n",
        "        \"\"\"\n",
        "        # 0th index = keyword embedding\n",
        "        keyword_emb = self.embedder.encode([keyword_text])[0]\n",
        "        # 1st index = full document embedding\n",
        "        doc_emb = self.embedder.encode([text])[0]\n",
        "\n",
        "        # Store as a small 2 x dim matrix: [0] keyword, [1] full doc\n",
        "        embeddings = np.stack([keyword_emb, doc_emb], axis=0)  # shape (2, dim)\n",
        "\n",
        "        doc_id = self._alloc_id()\n",
        "\n",
        "        self.meta[doc_id] = {\n",
        "            \"text\": text,\n",
        "            \"keyword_text\": keyword_text,\n",
        "            \"external_id\": external_id,\n",
        "            \"embeddings\": embeddings,   # here '0th index' is embeddings[0]\n",
        "        }\n",
        "\n",
        "        # Add to FAISS indexes (one vector per doc per index)\n",
        "        self.keyword_index.add_with_ids(\n",
        "            keyword_emb[None, :],\n",
        "            np.array([doc_id], dtype=\"int64\"),\n",
        "        )\n",
        "        self.doc_index.add_with_ids(\n",
        "            doc_emb[None, :],\n",
        "            np.array([doc_id], dtype=\"int64\"),\n",
        "        )\n",
        "\n",
        "        return doc_id\n",
        "\n",
        "    def search_local(\n",
        "        self,\n",
        "        query: str,\n",
        "        top_k: int = 5,\n",
        "        keyword_threshold: float = 0.4,\n",
        "    ) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Search only in local FAISS indexes.\n",
        "        Stage 1: compare query embedding with 0th-index (keyword) embeddings.\n",
        "        \"\"\"\n",
        "        q_emb = self.embedder.encode([query]).astype(\"float32\")  # shape (1, dim)\n",
        "\n",
        "        # Keyword-level FAISS search\n",
        "        D_kw, I_kw = self.keyword_index.search(q_emb, top_k)\n",
        "\n",
        "        sims = D_kw[0]\n",
        "        ids = I_kw[0]\n",
        "        mask = sims >= keyword_threshold\n",
        "\n",
        "        filtered_ids = ids[mask]\n",
        "        filtered_sims = sims[mask]\n",
        "\n",
        "        results = []\n",
        "        for doc_id, sim in zip(filtered_ids.tolist(), filtered_sims.tolist()):\n",
        "            if doc_id == -1:\n",
        "                continue\n",
        "            meta = self.meta.get(doc_id)\n",
        "            if not meta:\n",
        "                continue\n",
        "\n",
        "            results.append(\n",
        "                {\n",
        "                    \"doc_id\": doc_id,\n",
        "                    \"similarity_keyword\": float(sim),\n",
        "                    \"text\": meta[\"text\"],\n",
        "                    \"keyword_text\": meta[\"keyword_text\"],\n",
        "                    \"external_id\": meta[\"external_id\"],\n",
        "                }\n",
        "            )\n",
        "\n",
        "\n",
        "        return results\n",
        "    def search_with_pubmed_backfill(\n",
        "        self,\n",
        "        query: str,\n",
        "        pubmed_client: PubMedClient,\n",
        "        top_k: int = 5,\n",
        "        keyword_threshold: float = 0.4,\n",
        "    ) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Try local FAISS search; if nothing passes the threshold,\n",
        "        call PubMed, store new docs, and then search again.\n",
        "        \"\"\"\n",
        "        print(f\"Checking local vector store for query: '{query}'...\")\n",
        "\n",
        "\n",
        "        local_results = self.search_local(\n",
        "            query, top_k=top_k, keyword_threshold=keyword_threshold\n",
        "        )\n",
        "\n",
        "\n",
        "        if local_results:\n",
        "            print(f\" -> Found {len(local_results)} documents in local storage (Similarity >= {keyword_threshold}).\")\n",
        "            print(\" -> Skipping external PubMed search.\")\n",
        "            return local_results\n",
        "\n",
        "\n",
        "        print(\" -> No sufficient local matches found. Fetching from external PubMed API...\")\n",
        "\n",
        "\n",
        "        pubmed_docs = pubmed_client.fetch_docs(query)\n",
        "        print(f\" -> PubMed API retrieved {len(pubmed_docs)} documents.\")\n",
        "\n",
        "        initial_index_size = self.keyword_index.ntotal\n",
        "        print(f\" FAISS Index Size BEFORE adding: {initial_index_size}\")\n",
        "\n",
        "        for i, d in enumerate(pubmed_docs):\n",
        "            full_text = f\"{d.get('title', '')}\\n{d.get('abstract', '')}\"\n",
        "            self.add_document(\n",
        "                text=full_text,\n",
        "                keyword_text=query,\n",
        "                external_id=d.get(\"id\"),\n",
        "            )\n",
        "\n",
        "            print(f\" Successfully added document {i+1}/{len(pubmed_docs)}: PMID {d.get('id')}\")\n",
        "\n",
        "        final_index_size = self.keyword_index.ntotal\n",
        "        print(f\"  FAISS Index Size AFTER adding: {final_index_size}\")\n",
        "\n",
        "\n",
        "        return self.search_local(query, top_k=top_k, keyword_threshold=keyword_threshold)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KDefRb0oTx9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_rag_prompt(user_query: str, retrieved_docs: List[Dict[str, Any]]) -> str:\n",
        "\n",
        "    if not retrieved_docs:\n",
        "        return f\"User question: {user_query}\\nAnswer: I could not find any relevant documents to answer your question.\"\n",
        "\n",
        "\n",
        "    context_str = \"\"\n",
        "    for i, doc in enumerate(retrieved_docs, 1):\n",
        "        context_str += f\"Source {i}: [Title: {doc.get('title', 'Unknown')}]\\nContent: {doc['text']}\\n\\n\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are a research assistant. Answer the user question based ONLY on the provided Context sources.\n",
        "\n",
        "Instructions:\n",
        "1. You must base your answer strictly on the provided sources.\n",
        "2. You MUST cite the specific Source Number or Title when stating facts (e.g., \"According to Source 1...\").\n",
        "3. At the end of your answer, list the titles of the articles you used.\n",
        "\n",
        "Context:\n",
        "{context_str}\n",
        "\n",
        "User question:\n",
        "{user_query}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "    return prompt.strip()"
      ],
      "metadata": {
        "id": "yMP8UKuiTyAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LLMClient:\n",
        "    def __init__(self, model_id: str = \"/kaggle/working/Llama-2-7b-hf\"):\n",
        "        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "        self.model_id = model_id\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\",\n",
        "        )\n",
        "        self.pipe = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=self.model,\n",
        "            tokenizer=self.tokenizer,\n",
        "            device_map=\"auto\",\n",
        "        )\n",
        "\n",
        "    def generate(self, prompt: str, max_tokens: int = 256) -> str:\n",
        "        outputs = self.pipe(\n",
        "            prompt,\n",
        "            max_new_tokens=max_tokens,\n",
        "            do_sample=False,\n",
        "            eos_token_id=self.tokenizer.eos_token_id,\n",
        "        )\n",
        "        # text-generation pipeline returns a list of dicts with 'generated_text'\n",
        "        full_text = outputs[0][\"generated_text\"]\n",
        "        # Strip the original prompt, keep only the completion if needed\n",
        "        return full_text[len(prompt):].strip()\n"
      ],
      "metadata": {
        "id": "J3g642zvTyCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_query(\n",
        "    user_query: str,\n",
        "    doc_store: DocumentStore,\n",
        "    pubmed_client: PubMedClient,\n",
        "    llm_client: LLMClient,\n",
        "    top_k: int = 5,\n",
        "    keyword_threshold: float = 0.4,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    End-to-end:\n",
        "    - embed query\n",
        "    - search local FAISS (cache check)\n",
        "    - if no results, call PubMed API\n",
        "    - DOWNLOAD PDFS for the found articles (New Step)\n",
        "    - build RAG prompt\n",
        "    - generate answer\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    retrieved = doc_store.search_with_pubmed_backfill(\n",
        "      query=user_query,\n",
        "      pubmed_client=pubmed_client,\n",
        "      top_k=10,\n",
        "      keyword_threshold=keyword_threshold,\n",
        ")\n",
        "\n",
        "# Selecting  best 3 using reward model\n",
        "      top3_docs = select_top3_docs(user_query, retrieved, reward_model)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print(f\"\\n[System] Found {len(retrieved)} relevant abstracts.\")\n",
        "\n",
        "    # Initialize the downloader class we created earlier\n",
        "    downloader = PDFDownloader()\n",
        "\n",
        "    # Loop through every article found to get its full PDF\n",
        "    for doc in retrieved:\n",
        "        pmid = doc.get('external_id')    # Get the PubMed ID\n",
        "        title = doc.get('title', 'Untitled')\n",
        "\n",
        "        if pmid:\n",
        "            # This triggers the download (or skips if file exists)\n",
        "            downloader.download_pdf(pmid, title)\n",
        "\n",
        "    prompt = build_rag_prompt(user_query, top3_docs)\n",
        "    answer = llm_client.generate(prompt)\n",
        "\n",
        "    return answer\n",
        "\n"
      ],
      "metadata": {
        "id": "Yeb75PbjW_d5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "embedder = Embedder(EMBED_MODEL_NAME)\n",
        "dim = embedder.encode([\"test\"]).shape[1]   # infer embedding dimension\n",
        "\n",
        "doc_store = DocumentStore(embedder=embedder, dim=dim)\n",
        "pubmed_client = PubMedClient(max_results=3)\n",
        "llm_client = LLMClient()\n",
        "\n",
        "print(\"Embedding dimension:\", dim)\n"
      ],
      "metadata": {
        "id": "lGOV1jfZTyEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "faiss.write_index(doc_store.keyword_index, \"/kaggle/working/keyword.index\")\n",
        "faiss.write_index(doc_store.doc_index, \"/kaggle/working/doc.index\")\n",
        "\n",
        "import pickle, os\n",
        "with open(\"/kaggle/working/metadata.pkl\", \"wb\") as f:\n",
        "    pickle.dump(doc_store.meta, f)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "k57nBNk5fOxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_store.keyword_index = faiss.read_index(\"/kaggle/working/keyword.index\")\n",
        "doc_store.doc_index = faiss.read_index(\"/kaggle/working/doc.index\")\n",
        "\n",
        "import pickle\n",
        "with open(\"/kaggle/working/metadata.pkl\", \"rb\") as f:\n",
        "    doc_store.meta = pickle.load(f)\n",
        "\n",
        "doc_store._next_local_id = max(doc_store.meta.keys(), default=0) + 1\n"
      ],
      "metadata": {
        "id": "VvbAvs30fSoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use only the search keywords\n",
        "search_term = \"diabetes caused by sugar\"\n",
        "\n",
        "print(\"--- RUNNING QUERY (Clean Search Term) ---\")\n",
        "answer = answer_query(\n",
        "    # Only send keywords to the search/retrieval system\n",
        "    user_query=search_term,\n",
        "    doc_store=doc_store,\n",
        "    pubmed_client=pubmed_client,\n",
        "    llm_client=llm_client,\n",
        "    top_k=3,\n",
        "    keyword_threshold=0.4,\n",
        ")\n",
        "\n",
        "print(\"\\nUser search term:\")\n",
        "print(search_term)\n",
        "print(\"\\nSystem answer:\")\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "f14bDg9fZfNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gYgN81_oZfSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-yTpJQE-ZfUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9WlGBVZGZfWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ut1zgUajZfZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7pHmdCu1ZfbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UGjlEV55ZfdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cw9OVJukZfiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ccyA5MoPoLQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ceBF6nGSoLS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WvlRaBp_qXcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "doPLcq40qXeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-hdgnZo7qXgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xwXqPldlqXit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gDngrlSGqXkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9k5DXbyxqXnA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}