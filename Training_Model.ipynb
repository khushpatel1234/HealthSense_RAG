{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKkP74Git-c0"
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers accelerate peft trl datasets sentence-transformers faiss-cpu\n",
        "\n",
        "!pip install sentence-transformers torch transformers datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#reward model training\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sentence_transformers import CrossEncoder\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "class PairwiseRewardDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Each item: query + positive doc + negative doc\n",
        "    \"\"\"\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        return item[\"query\"], item[\"pos_doc\"], item[\"neg_doc\"]\n",
        "\n",
        "\n",
        "train_data = []\n",
        "\n",
        "\n",
        "dataset = PairwiseRewardDataset(train_data)\n",
        "train_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "\n",
        "model_name = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
        "reward_model = CrossEncoder(model_name, num_labels=1)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "reward_model.to(device)\n",
        "\n",
        "margin = 1.0\n",
        "loss_fn = nn.MarginRankingLoss(margin=margin)\n",
        "optimizer = torch.optim.Adam(reward_model.parameters(), lr=2e-5)\n",
        "\n",
        "\n",
        "epochs = 15\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for batch in train_loader:\n",
        "        queries, pos_docs, neg_docs = batch\n",
        "\n",
        "\n",
        "        inputs_pos = [(q, d) for q, d in zip(queries, pos_docs)]\n",
        "        inputs_neg = [(q, d) for q, d in zip(queries, neg_docs)]\n",
        "\n",
        "\n",
        "        scores_pos = reward_model.predict(inputs_pos)\n",
        "        scores_neg = reward_model.predict(inputs_neg)\n",
        "\n",
        "        scores_pos = torch.tensor(scores_pos, dtype=torch.float32, device=device)\n",
        "        scores_neg = torch.tensor(scores_neg, dtype=torch.float32, device=device)\n",
        "\n",
        "        # Target: pos > neg â†’ y = 1\n",
        "        y = torch.ones_like(scores_pos, device=device)\n",
        "\n",
        "        loss = loss_fn(scores_pos, scores_neg, y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "reward_model.save(\"trained_reward_model\")\n",
        "\n",
        "\n",
        "def select_top3_docs(query: str, docs: list, reward_model):\n",
        "    \"\"\"\n",
        "    docs: List of dicts with 'text', 'title', 'external_id'\n",
        "    Returns top-3 docs based on reward model scores\n",
        "    \"\"\"\n",
        "    if len(docs) <= 3:\n",
        "        return docs  # nothing to select\n",
        "\n",
        "    pairs = [(query, doc['text']) for doc in docs]\n",
        "    scores = reward_model.predict(pairs)\n",
        "    top3_idx = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:3]\n",
        "    top3_docs = [docs[i] for i in top3_idx]\n",
        "    return top3_docs\n",
        "\n",
        "\n",
        "retrieved_docs = [\n",
        "]\n",
        "\n",
        "query = \"\"\n",
        "top3_docs = select_top3_docs(query, retrieved_docs, reward_model)\n",
        "print(\"Top 3 documents selected by reward model:\")\n",
        "for doc in top3_docs:\n",
        "    print(doc['title'], \"-\", doc['text'][:50])\n",
        "\n"
      ],
      "metadata": {
        "id": "s87hWVbVxKQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Llama base model SFT.\n",
        "\n",
        "import torch\n",
        "from transformers import LlamaForCausalLM, LlamaTokenizer, Trainer, TrainingArguments\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load base LLaMA 7B\n",
        "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
        "model = LlamaForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    load_in_8bit=True,  # VRAM-efficient\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# LoRA config (small adapter for structure learning)\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Load template dataset (structured output only)\n",
        "dataset = load_dataset(\"json\", data_files=\"template_dataset.json\")[\"train\"]\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize(example):\n",
        "    input_text = f\"Query: {example['query']}\\nContext: {example['context']}\\nAnswer:\"\n",
        "    target_text = example['output']\n",
        "    input_ids = tokenizer(input_text, truncation=True, padding=\"max_length\", max_length=256).input_ids\n",
        "    labels = tokenizer(target_text, truncation=True, padding=\"max_length\", max_length=128).input_ids\n",
        "    return {\"input_ids\": input_ids, \"attention_mask\": [1]*len(input_ids), \"labels\": labels}\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize, remove_columns=dataset.column_names)\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./sft_llama7b\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    num_train_epochs=1,  # only template learning\n",
        "    learning_rate=1e-4,\n",
        "    fp16=True,\n",
        "    logging_steps=5,\n",
        "    save_steps=100,\n",
        "    save_total_limit=1\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "model.save_pretrained(\"./sft_llama7b\")\n"
      ],
      "metadata": {
        "id": "QsvfYSZnwGiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reward_fn(output, context_articles):\n",
        "    \"\"\"\n",
        "    Rule-based reward:\n",
        "    - 1 point for classification\n",
        "    - up to 2 points for citing top-3 articles\n",
        "    - 1 point for structured format\n",
        "    \"\"\"\n",
        "    reward = 0.0\n",
        "    max_reward = 3.0\n",
        "\n",
        "    # Classification\n",
        "    if \"pseudoscience\" in output.lower() or \"scientific fact\" in output.lower():\n",
        "        reward += 1.0\n",
        "\n",
        "    # Citing top-3 context\n",
        "    cited_count = sum(1 for article in context_articles if article.lower() in output.lower())\n",
        "    reward += min(cited_count, 2)\n",
        "\n",
        "    # Structured output\n",
        "    if (\"classification\" in output.lower() or \"the claim is\" in output.lower()) and \\\n",
        "       (\"based on\" in output.lower() or \"according to\" in output.lower()):\n",
        "        reward += 1.0\n",
        "\n",
        "    return min(reward / max_reward, 1.0)\n"
      ],
      "metadata": {
        "id": "ovAAudpywxnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LLama model GRPO\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
        "\n",
        "# Load the SFT-finetuned model for GRPO\n",
        "model = AutoModelForCausalLMWithValueHead.from_pretrained(\"./sft_llama7b\")\n",
        "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# PPO configuration\n",
        "ppo_config = PPOConfig(\n",
        "    batch_size=1,\n",
        "    forward_batch_size=1,\n",
        "    ppo_epochs=4,\n",
        "    log_with=None\n",
        ")\n",
        "\n",
        "ppo_trainer = PPOTrainer(\n",
        "    config=ppo_config,\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    dataset=None  # manual stepping\n",
        ")\n",
        "\n",
        "# Dataset wrapper\n",
        "class PPODataset(Dataset):\n",
        "    def __init__(self, queries, top3_contexts):\n",
        "        \"\"\"\n",
        "        queries: list of user queries\n",
        "        top3_contexts: list of top-3 docs per query (already selected by reward model)\n",
        "        \"\"\"\n",
        "        self.queries = queries\n",
        "        self.contexts = top3_contexts\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.queries)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\"query\": self.queries[idx], \"context\": self.contexts[idx]}\n",
        "\n",
        "\n",
        "queries = []\n",
        "top3_contexts = []\n",
        "ppo_dataset = PPODataset(queries, top3_contexts)\n",
        "\n",
        "\n",
        "group_size = 5\n",
        "for i in range(0, len(ppo_dataset), group_size):\n",
        "    group = [ppo_dataset[j] for j in range(i, min(i+group_size, len(ppo_dataset)))]\n",
        "\n",
        "    input_ids_list = []\n",
        "    context_list = []\n",
        "\n",
        "    # Prepare inputs\n",
        "    for item in group:\n",
        "        query = item[\"query\"]\n",
        "        context = item[\"context\"]\n",
        "        context_list.append(context)\n",
        "        input_text = f\"Query: {query}\\nContext: {context}\\nAnswer:\"\n",
        "        input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.cuda()\n",
        "        input_ids_list.append(input_ids)\n",
        "\n",
        "    # Generate outputs\n",
        "    outputs_list = [model.generate(ids, max_new_tokens=128) for ids in input_ids_list]\n",
        "    decoded_list = [tokenizer.decode(out[0], skip_special_tokens=True) for out in outputs_list]\n",
        "\n",
        "    # Compute group-level reward\n",
        "    rewards = [reward_fn(decoded, ctx) for decoded, ctx in zip(decoded_list, context_list)]\n",
        "    group_reward = sum(rewards) / len(rewards)\n",
        "\n",
        "    # PPO step for each item in the group\n",
        "    for input_ids, outputs in zip(input_ids_list, outputs_list):\n",
        "        ppo_trainer.step(input_ids, outputs, group_reward)\n",
        "\n",
        "model.save_pretrained(\"./grpo_llama7b\")\n"
      ],
      "metadata": {
        "id": "oMng3g1yuG4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f-9ICVy6uG6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w2ql_B2UuG83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4r3sJJ_TuG-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9vYzfEp0xJfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AejfEO2muHBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4f-kVh3UuHDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3xWDg7y4uHFV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}